{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "W327fKLBB4R7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import random\n",
        "import urllib.request\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Optional\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the NLTK Gutenberg corpus\n",
        "nltk.download('gutenberg')\n",
        "# Download the punkt resource - Punkt is a pre-trained sentence tokenizer model.\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6PlayQjCvLa",
        "outputId": "083829b6-5002-431c-d145-54f5f0ff4bb9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel:\n",
        "    def __init__(self, text: str) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the BigramModel with the provided text.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input text for building the bigram model.\n",
        "        \"\"\"\n",
        "        # 1. Tokenize the text into individual words:\n",
        "        self.words = nltk.word_tokenize(text) # - `nltk.word_tokenize(text)`: This function from the NLTK library splits the input text into a list of words (tokens), considering punctuation and word boundaries.\n",
        "        # 2. Create a list of bigrams (pairs of consecutive words):\n",
        "        self.bigrams = list(zip(self.words[:-1], self.words[1:])) # - `zip(self.words[:-1], self.words[1:])`: This combines pairs of words from the `self.words` list, taking the first word with the second, the second with the third, and so on.\n",
        "                                                                  # - `[:-1]`: This slicing notation excludes the last word to ensure equal-length pairs.\n",
        "        # 3. Initialize a dictionary to store bigram frequencies:\n",
        "        self.bigram_model = defaultdict(list) # - `defaultdict(list)`: This creates a special dictionary where accessing a non-existent key automatically creates an empty list for that key. This is convenient for counting occurrences of bigrams.\n",
        "        # 4. Build the bigram model\n",
        "        self._build_bigram_model()\n",
        "\n",
        "    def _build_bigram_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Build the bigram model from the provided text.\n",
        "        \"\"\"\n",
        "        # Iterate through each bigram (pair of consecutive words):\n",
        "        for bigram in self.bigrams:\n",
        "        # 1. Access the list of second words associated with the first word:\n",
        "          second_words_list = self.bigram_model[bigram[0]]\n",
        "\n",
        "          # 2. Append the second word of the bigram to the list:\n",
        "          second_words_list.append(bigram[1])\n",
        "\n",
        "          # 3. Update the dictionary with the updated list:\n",
        "          self.bigram_model[bigram[0]] = second_words_list\n",
        "\n",
        "    def generate_sentence(self, start_word: Optional[str] = None, num_words: int = 15) -> str:\n",
        "      \"\"\"\n",
        "      Generates a sentence using the bigram model by randomly selecting words based on their frequencies.\n",
        "      \"\"\"\n",
        "\n",
        "      # 1. Handle optional starting word:\n",
        "      if start_word is None:\n",
        "          # If not provided, choose a random word from the text:\n",
        "          start_word = random.choice(self.words)\n",
        "\n",
        "      # 2. Initialize sentence and current word:\n",
        "      sentence = [start_word]  # Begin the sentence with the chosen starting word\n",
        "      current_word = start_word  # Set the current word to the starting word\n",
        "\n",
        "      # 3. Generate subsequent words:\n",
        "      for _ in range(num_words - 1):  # Repeat for the desired number of words (excluding the starting word)\n",
        "          if current_word in self.bigram_model:\n",
        "              # If the current word is in the bigram model:\n",
        "              next_word = random.choice(self.bigram_model[current_word])  # Choose a random word that followed it in the text\n",
        "              sentence.append(next_word)  # Add the chosen word to the sentence\n",
        "              current_word = next_word  # Update the current word for the next iteration\n",
        "          else:\n",
        "              # If the current word is not in the bigram model, stop generating to avoid errors\n",
        "              break\n",
        "\n",
        "      # 4. Join the words into a sentence:\n",
        "      return \" \".join(sentence)  # Combine the words into a string with spaces\n"
      ],
      "metadata": {
        "id": "JMZsKtmTC0E1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_text(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Download a text file from a given URL.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the text file.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the text file.\n",
        "    \"\"\"\n",
        "    response = urllib.request.urlopen(url)\n",
        "    return response.read().decode('utf-8')"
      ],
      "metadata": {
        "id": "B6QJOcUtGLZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "url = 'https://www.gutenberg.org/files/11/11-0.txt'\n",
        "text = download_text(url)"
      ],
      "metadata": {
        "id": "8cT16WDdC2vY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model = BigramModel(text)"
      ],
      "metadata": {
        "id": "szOWjK03C5i1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_word = 'I'\n",
        "generated_sentence = bigram_model.generate_sentence(start_word)\n",
        "print(generated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_EslbMjC-Ko",
        "outputId": "3b3159ca-a325-453e-a3b3-0f4695259a77"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ! ‚Äù said the court of the eyes , hurried off , now and\n"
          ]
        }
      ]
    }
  ]
}